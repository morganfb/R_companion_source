%!TEX root=../../main.tex

%% FIGURE NUMBERs WILL LIKELY CHANGE WHEN CH 7 IS FINALIZED, SO CHECK THEM ALL AT SOME POINT 

In Chapter 7, the dataset \texttt{statins} was introduced when visually understanding the assumptions of simple linear regression.  It will again be used in this chapter to understand the idea of multiple linear regression.

%% Should I include Figure 7.2 in here, seems uneccessary.  


\section*{Confounders} 

<<fig.height=4, fig.width=5,fig.align = "center", tidy = TRUE>>=
## Figure 7.1 
# First collecting the data sample 
set.seed(5011)
row.num = sample(1:nrow(statins), 500, replace = FALSE)
statins.samp = statins[row.num, ]

# Make the plot where color is specified by Statin use 
plot(statins.samp$Age, statins.samp$RFFT, pch = 19, cex = 1.3, col= as.factor(statins.samp$Statin), xlab = "Age (yrs)", ylab = "RFFT Score")
@

\textit{OI Biostat} walks through the example of statin use as a confounder because it is theoretically associated with both another explanatory variable, in this case age, and with a response variable, here cognitive decline.  To test for a confounder, a measure of association can be used such as correlation or a side-by-side boxplot.  In this example, the correlations between the variables can be calculated as follows.  

<<fig.height=4, fig.width=3,fig.align = "center", tidy = TRUE>>=
## Figure 7.4 
# Visual test of association
boxplot(statins.samp$Age ~ statins.samp$Statin, ylab = "Age (yrs)", xlab = "Statin Use")

# Numerical test of association 
cor(statins.samp$Statin, statins.samp$Age)
cor(statins.samp$Statin, statins.samp$RFFT)
@

A correlation greater than 0.3 or less than -0.3 is considered signficant.  Note that these correlations are not immediately concerning as only one is mildly signficant, but nonetheless, they could be impactful on an investigation and should be accounted for if possible.  The conclusion in the main text is thus that a multiple linear regression should be used because it can adjust for confounders.  The explanation of how this works is complicated, but the idea is that because multiple linear regression builds a model that reflects the association between explanatory and response variables, the more information included in building this model, the better.  

\stepcounter{section}
\section{Simple vs. Multiple Regression}
Firstly, the simple linear regression can be performed as would have been done in Chapter 6.  
<<tidy = TRUE>>=
summary(lm(statins.samp$RFFT ~ statins.samp$Statin))
@
This gives the following model, 
\[ RFFT = 70.714 - 10.053\cdot Statin \]

Next, the multiple regression will be performed. Note the use of a plus sign after the tilde to include multiple variables.   
<<tidy = TRUE>>=
summary(lm(statins.samp$RFFT ~ statins.samp$Statin + statins.samp$Age))
@
This gives the following model, 
\[ RFFT = 137.8822 + 0.8509\cdot Statin - 1.2710 \cdot Age\]

As compared to a model with statin use as the only explanatory variable, it is evident that the coefficients here change quite drastically by including both explanatory variables.  This is reflective of the confounding relationship between the two explanatory variables, but will hopefully lead us to more accurate results.  

Notice how in this example, the first model shows a negative relationship between statin use and RFFT values, while the multiple regression shows a positive relationship.  This is reflective that the \texttt{Statin} variable is representative of different things in the two models.  In the simple model, the coefficient for statin use represents the difference between the mean of RFFT value for the two groups, statin users and non-users.  This calculation can be seen directly from the sample below.  However, the coefficient in the multiple regression model is significantly more complicated.  It represents the average difference in means for statin users and non-users of the same age.  

<<>>=
# Calculation of the simple linear regression coefficient
mean(statins.samp$RFFT[statins.samp$Statin == 1]) - mean(statins.samp$RFFT[statins.samp$Statin == 0])
@

\section{Evaluating The Fit of a Multiple Regression Model}
\subsection{Assumptions of Linear Regression}
The same procedures as outlined in Chapter 6 can be used to test the model assumptions, and will be repeated here.  
\begin{enumerate}
\item \textbf{Linearity:}  The plots below show that the relationship of the response variable with each explanatory variable is approximately linear.    
<<fig.height=4, fig.width=7.5,fig.align = "center", tidy = TRUE>>=
par(mfrow=c(1,2))
plot(statins.samp$RFFT ~ statins.samp$Age, cex = 0.6, ylab = "RFFT Score", xlab = "Age (yrs)", main = "") 
abline(lm(statins.samp$RFFT ~ statins.samp$Age), col = "red")
plot(statins.samp$RFFT ~ statins.samp$Statin, cex = 0.6, ylab = "RFFT Score", xlab = "Age (yrs)", main = "") 
abline(lm(statins.samp$RFFT ~ statins.samp$Statin), col = "red")
@

\item \textbf{Constant Variability:} Note that the assumption is approximately constant variance in the \texttt{residuals}, which can be investigated with the following, showing that the residuals do appear to have constant variance.  
<<fig.height=4, fig.width=4,fig.align = "center", tidy = TRUE>>=
residuals = resid(lm(statins.samp$RFFT~statins.samp$Age + statins.samp$Statin))
plot(residuals, ylab = "Residuals")
@

\item \textbf{Independent Observations:} This assumption remains dependent on the data collection itself, and based on the parameters of the study, the data is believed to be independent.  

\item \textbf{Residuals that are approximately normally distributed:} This can be investigated with a historgram or a normal q-q plot as follows.  The residuals do appear to be approximately normally distributed.  
<<fig.height=4, fig.width=7.5,fig.align = "center", tidy = TRUE>>=
par(mfrow=c(1,2))
hist(residuals, xlab = "Residuals", freq = FALSE)
x <- seq(min(residuals) - 2, max(residuals) + 2, 0.01)
y <- dnorm(x, mean(residuals), sd(residuals))
lines(x, y, lwd = 1.5, col = "red")

qqnorm(residuals)
qqline(residuals)
@

\end{enumerate}

\subsection{Hypothesis Tests and Confidence Intervals}
\textsf{R} makes the process of performing hypothesis tests on coefficients of the multiple linear regression model very simple, as it performs all the tests and automatically returns these in the summary of the linear regression.  As with simple linear regression, the \textsf{R} output of a multiple regression provides a t-statistic and a corresponding p-value for the hypothesis test of 
\[ H_0: \beta_k = 0 \text{vs. } H_A: \beta_k \neq 0 \]

Furthermore, the summary output also includes the F-statistic for the hypothesis 
\[ H_0: \beta_1 = \beta_2 = \cdots = \beta_p = 0 \]
In words, this tests if any of the coefficients in combination are useful at predicting the response variable.  Note that this tests if any of the coefficients is non-zero, not if all of them are non-zero.  

\stepcounter{section}
\section{Categorical Predictors With More Than Two Values}
The function \texttt{as.factor()} has previously been used to tell \textsf{R} to treat the numerical values of a variable as frivolous, rather representing different categories.  In other words, a value of 1 for statin does not imply one unit of statins were used by the individual, rather that any amount of statins were used.  Data is often stored with numerical values for ease of manipulation, even if the numerical values do not have significant meaning.  In these cases, it is important to utilize the function \texttt{as.factor()} to clarify to \textsf{R} whether the numbers have meaning as numbers or as categories, with \texttt{as.factor()} implying that they are categories, or factors.  

Using this idea on the statin sample data, the following simple regression is obtained, with the reference category being a value of 0 for \texttt{Statin} since it is the one that is not printed in the output.  
<<>>=
summary(lm(statins.samp$RFFT ~ as.factor(statins.samp$Statin)))
@
In this example, the regression using \texttt{Statin} as either a numerical value or as a categorical variable is the same because of the method by which it was encoded.  Using binary values, i.e. 0 and 1, signals to \textsf{R} that a variable is a categorical variable, and this is mathematically equivalent.  Where this becomes significantly more complicated is when there are more than two categories because \textsf{R} can no longer make the assumption of categorical variable and will assume non-categorical values with numerical significance.  

An example of where this is a problem is as follows, 
<<>>=
## NOTE, this method is INCORRECT and provided for example,
## so please be careful copying 
summary(lm(statins.samp$RFFT ~ statins.samp$Education))

## Correct method 
summary(lm(statins.samp$RFFT ~ as.factor(statins.samp$Education)))
@

Notice that for an individual with an education level of 2, the first \textit{incorrect} model predicts an RFFT score of 71.464 while the second \textit{correct} model predicts a score of 73.1074.  In this example, the difference is not massive but certainly of concern, so use serious caution when working with categorical variables in \textsf{R}.  

\subsection{Using ANOVA For Categorical Variables}
When using the ANOVA for categorical variables, it is important to keep in mind that they must again be treated correctly as factors and not as numerical values.  An example of the above model treated using an ANOVA is given below.  
<<>>=
anova(lm(statins.samp$RFFT ~ as.factor(statins.samp$Education)))
@

\subsection{Using Dummy Variables} 
Another method for considering categorical variables is the use of a dummy variable, which is a binary indicator if a value is equal to another.  In other words, a model can use several dummy variables, each of which represents a category and is equal to 1 if that individual is in the specified category and 0 otherwise.  This can be performed as follow, where the categorical value which is not seprataly specified is the reference variable.  Note that this method gives the same results as the linear regression above using \texttt{as.factor()} 

<<>>=
ed.dummy.1 = statins.samp$Education == 1
ed.dummy.2 = statins.samp$Education == 2
ed.dummy.3 = statins.samp$Education == 3
summary(lm(statins.samp$RFFT ~ ed.dummy.1 + ed.dummy.2 + ed.dummy.3))
@

\section{Analysis of the Statin Dataset}
Performing the more complex multiple regression can be done as follows, 
<<>>=
summary(lm(statins.samp$RFFT ~ as.factor(statins.samp$Statin) + statins.samp$Age + as.factor(statins.samp$Education) + as.factor(statins.samp$CVD)))
@

Testing the assumptions and evaluating the fit, the model is reasonably good, but by no means perfect.  The $R^2$ value is only 0.4355, which is an improvement, and the plots below show fairly normal residuals. Note that a perfect model would be highly suspcious because of the nature of data analysis, so the goal is to find the best model possible without finding something suspicious.  
<<fig.height=4, fig.width=7.5,fig.align = "center", tidy = TRUE>>=
residuals = resid(lm(statins.samp$RFFT ~ as.factor(statins.samp$Statin) + statins.samp$Age + as.factor(statins.samp$Education) + as.factor(statins.samp$CVD)))
par(mfrow=c(1,2))
hist(residuals, xlab = "Residuals", freq = FALSE)
x <- seq(min(residuals) - 2, max(residuals) + 2, 0.01)
y <- dnorm(x, mean(residuals), sd(residuals))
lines(x, y, lwd = 1.5, col = "red")

qqnorm(residuals)
qqline(residuals)
@

\section{Analysis of the FAMuSS Dataset}
Going back to the FAMuSS dataset, a more comprehensive study can now be performed using multiple linear regression to account for various confounders.  As a baseline, two different simple linear regressions can be computed as follows, the goal of the following analysis to be to improve upon this simple model.  

<<>>=
## Simple genotype model - significant
summary(lm(log(famuss$ndrm.ch+5) ~ famuss$actn3.r577x))

## Simple age model - significant 
summary(lm(log(famuss$ndrm.ch+5) ~ famuss$age))

## Simple bmi model -significant
summary(lm(log(famuss$ndrm.ch+5) ~ famuss$bmi))

## Simple sex model - significant
summary(lm(log(famuss$ndrm.ch+5) ~ famuss$sex))

## Simple race model - insignificant
summary(lm(log(famuss$ndrm.ch+5) ~ famuss$race))
@

One question that may come up from the following line of code is why the gene actn3.577x, the sex, or the race are not coded as a factor in this regression line, and the simple answer is that the dataset already has these variables coded as a factor.  To test if this is the case or not, the function \texttt{class()} can be used, which will tell the variable type.  

<<>>=
class(famuss$actn3.r577x)
@

Now, using the information based on the single regressions, a multiple regression can be built.  Both the residuals and the $R^2$ indicate that this model is far from great.  The main text talks through why this inaccuracy could be caused by data collection error and is unlikely to be an indication of fallacy in the methods.  
<<fig.height=4, fig.width=7.5,fig.align = "center", tidy = TRUE>>=
summary(lm(log(famuss$ndrm.ch+5) ~ famuss$actn3.r577x + famuss$age + famuss$sex+ famuss$bmi))

residuals = resid(lm(log(famuss$ndrm.ch+5) ~ famuss$actn3.r577x + famuss$age + famuss$sex+ famuss$bmi))
par(mfrow=c(1,2))
hist(residuals, xlab = "Residuals", freq = FALSE)
x <- seq(min(residuals) - 2, max(residuals) + 2, 0.01)
y <- dnorm(x, mean(residuals), sd(residuals))
lines(x, y, lwd = 1.5, col = "red")

qqnorm(residuals)
qqline(residuals)
@


\section{Introduction to Logistic Regression}
Section 8.4 in \textit{OI Biostat} introduces logistic regression, a method used to create models with a binary response variable.  