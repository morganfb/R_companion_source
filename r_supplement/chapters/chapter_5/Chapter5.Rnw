\vspace{0.5cm} 

\section{The t-distribution}
The main text introduces the \textbf{t-distribution} and explains under what circumstances it should be used.  Using it in in R is very similar to many concepts seen through Chapter 4 thus far.  

The following plot shows how the t-distribution approximates the normal distribution as the degrees of freedom grow.  The use of the function \textit{dt()} parallels the use of \textit{dnorm()} as has been used in previous chapters.  

<<fig.height=4, fig.width=6,fig.align = "center", tidy = TRUE>>=
## Figure 5.2 
# getting the x-axis values 
x = seq(-5, 9, 0.1)

# finding the corresponding distribution values for several distributions 
t1 = dt(x, df = 1)
t2 = dt(x, df = 2)
t4 = dt(x, df = 4)
t8 = dt(x, df = 8)
norm = dnorm(x)

# plotting the results, with custom colors 
plot(x,t1, type = "l", ylim = c(0, 0.4), xlab = "", ylab = "", col = rgb(0,0,1,alpha=0.1))
lines(x, t2, col = rgb(0,0,1,alpha=0.2))
lines(x, t4, col = rgb(0,0,1,alpha=0.3))
lines(x, t8, col = rgb(0,0,1,alpha=0.4))
lines(x, norm, col = rgb(0,0,1,alpha=0.5))
@

\subsection{The Alternative to t-Tables}
The main text talks through the use of a \textbf{t-table}, but this text will go through examples of using software to do the same calculations.  In $R$, the command of interests here are \textit{pt()} to get the percentage of the distribution above or below a certain value and \textit{qt()} to get distribution values for which a certain percentage of the distribution falls above.  The command \textit{pt()} takes values inside the t-table to return the values on the x-axis of the table, whereas \textit{qt()} returns values within the table when given values along the upper x-axis.  

An example of a one-sided calculation using the t-distribution can be viewed in Example 5.1 below.  Note the use of the stipulation \textit{lower.tail = TRUE} which ensures that the calculation is of values in the lower tail, i.e. those values which are less than -2.1.  
<<>>=
## Example 5.1 
pt(-2.1, df = 18, lower.tail = TRUE)
@

Example 5.2 shows a two sided calculation using the t-distribution.  In order to do this in $R$, each tail can be calculated separately and then the two values summed.  Alternatively, the symmetry of the t-distribution can be utilized to calculate one tail and multiply it by two.  
<<>>=
## Example 5.2 
# Method 1: calculate each tail and sum 
pt(1.65, df = 20, lower.tail = FALSE) + pt(-1.65, df = 20, lower.tail = TRUE)

# Method 2: calculate one tail and multiply by 2 
2*pt(1.65, df = 20, lower.tail = FALSE)
@


\subsection{Confidence Intervals}
As in chapter 4, confidence intervals using the t-distribution can be set up in the same way with a $t^*$ value instead of a $z^*$ value.  

<<>>=
## Example 5.3 
n = 19 
xbar = 4.4 
s = 2.3 
std.error = s/sqrt(n)
perc = .95 
t.star = qt(p = perc + (1-perc)/2, df = n-1, lower.tail = TRUE)
ci = c(xbar - t.star*std.error, xbar + t.star*std.error)
ci
@

<<>>=
## Example 5.5 
mu = 0.5 
n = 15 
xbar = 0.287 
s = 0.069
std.error = s/sqrt(n)
t = (xbar - mu)/std.error
pt(t, df = n-1)
@

\section{Paired Data} 
When working with paired data, several methods can be used to measure the effect.  The first method is to calculate by hand differences between the pairs and then to perform a standard t-test as follows.  
<<>>=
## Example 5.6 
diff = swim$wet.suit.velocity - swim$swim.suit.velocity
t.test(diff, mu = 0, alternative = "two.sided")
@

The second method involves using a \textbf{paired t-test}, which performs the same operation without the direct calculation of the differences being required first.  It can be seen that the output of this test is the exact same as the test above.  This is a nice example of $R$ taking out a lot of the computational work.  
<<>>=
t.test(swim$wet.suit.velocity, swim$swim.suit.velocity, paired = TRUE)
@

\section{Difference of Two Means} 
%% COME BACK HERE 


\subsection{Hypothesis Tests for a Different in Means}
In order to perform a \textbf{two-sample t-test}, there are again two methods.  The first involves dividing the data into two samples (unless the data has already come separated) and performing a t-test.  The second has $R$ do the entire process.  Note that for this method, the tilde signifies the grouping variable.  So in this case, the \textit{weight} data is grouped by the variable \textit{smoke}.  
<<>>=
## Example 5.11 
# Method 1: divide the data by hand 
smoker = baby.smoke$weight[baby.smoke$smoke == "smoker"]
non.smoker = baby.smoke$weight[baby.smoke$smoke == "nonsmoker"]
t.test(non.smoker, smoker)

# Method 2: R does it all 
t.test(baby.smoke$weight~baby.smoke$smoke)
@

\subsection{Pooled t-test} 
Performing what is known as a \textbf{pooled t-test} is very simple in $R$.  The procedure is the same as a two-sample t-test, with the addition of the clause \textit{var.equal = TRUE} in the \textit{t.test()} command.  This is because a pooled t-test relies on the assumption of nearly equal variance between the two samples.  For example, performing the same test as above but using a pooled t-test would look like the following, 
<<>>=
t.test(baby.smoke$weight~baby.smoke$smoke, var.equal = TRUE)
@


%% WHAT SHOULD I DO ABOUT ALL OF THE POWER STUFF??? NOT REALLY SURE R IS APPROPRIATE FOR A LOT OF THIS 

\stepcounter{section}
\section{ANOVA} 
The main text walks through the arithmetic procedures of the ANOVA test and calculating the F-statistic.  $R$ makes this process much simpler, such that the entire calculation can be performed with just one line of code.  The command \textit{anova()} takes as its input a linear regression model.  Chapter 6 will go into much more detail about linear regression, but for now, note that within the function \textit{lm()}, the first variable before the tilde is the one for which the means are being compared, while the second variable after the tilde specifies the groups.  The command below works out Example 5.22 from the main text, getting the same $F$ statistic as the hand calculated value.  

<<>>=
## Example 5.22 
anova(lm(famuss$ndrm.ch ~ famuss$actn3.r577x))
@